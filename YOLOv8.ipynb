{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511e5c4b",
   "metadata": {},
   "source": [
    "# Setup Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1a82b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'AS-One'...\n",
      "Updating files:  96% (428/445)\n",
      "Updating files:  97% (432/445)\n",
      "Updating files:  98% (437/445)\n",
      "Updating files:  99% (441/445)\n",
      "Updating files: 100% (445/445)\n",
      "Updating files: 100% (445/445), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/augmentedstartups/AS-One.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd665a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ultralytics\n",
      "Version: 8.0.98\n",
      "Summary: Ultralytics YOLOv8 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.\n",
      "Home-page: https://github.com/ultralytics/ultralytics\n",
      "Author: Ultralytics\n",
      "Author-email: hello@ultralytics.com\n",
      "License: AGPL-3.0\n",
      "Location: d:\\programs\\anaconda\\lib\\site-packages\n",
      "Requires: thop, opencv-python, requests, torch, PyYAML, pandas, psutil, seaborn, Pillow, sentry-sdk, tqdm, matplotlib, torchvision, scipy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip show ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22df02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kools\\Downloads\\Grad\\YOLO V8\\AS-One\n",
      "Collecting cython-bbox\n",
      "  Using cached cython_bbox-0.1.3.tar.gz (41 kB)\n",
      "Building wheels for collected packages: cython-bbox\n",
      "  Building wheel for cython-bbox (setup.py): started\n",
      "  Building wheel for cython-bbox (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for cython-bbox\n",
      "Failed to build cython-bbox\n",
      "Installing collected packages: cython-bbox\n",
      "    Running setup.py install for cython-bbox: started\n",
      "    Running setup.py install for cython-bbox: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'D:\\Programs\\ANACONDA\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\kools\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2spob2lx\\\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\kools\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2spob2lx\\\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\kools\\AppData\\Local\\Temp\\pip-wheel-ws7wfjn_'\n",
      "       cwd: C:\\Users\\kools\\AppData\\Local\\Temp\\pip-install-2spob2lx\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\n",
      "  Complete output (11 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'cython_bbox' extension\n",
      "  creating build\n",
      "  creating build\\temp.win-amd64-3.9\n",
      "  creating build\\temp.win-amd64-3.9\\Release\n",
      "  creating build\\temp.win-amd64-3.9\\Release\\src\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.33.31629\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -ID:\\Programs\\ANACONDA\\lib\\site-packages\\numpy\\core\\include -ID:\\Programs\\ANACONDA\\include -ID:\\Programs\\ANACONDA\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.33.31629\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.33.31629\\ATLMFC\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /Tcsrc/cython_bbox.c /Fobuild\\temp.win-amd64-3.9\\Release\\src/cython_bbox.obj -Wno-cpp\n",
      "  cl : Command line error D8021 : invalid numeric argument '/Wno-cpp'\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.33.31629\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for cython-bbox\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'D:\\Programs\\ANACONDA\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\kools\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2spob2lx\\\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\kools\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2spob2lx\\\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\kools\\AppData\\Local\\Temp\\pip-record-bafmvxig\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\Programs\\ANACONDA\\Include\\cython-bbox'\n",
      "         cwd: C:\\Users\\kools\\AppData\\Local\\Temp\\pip-install-2spob2lx\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\n",
      "    Complete output (13 lines):\n",
      "    running install\n",
      "    D:\\Programs\\ANACONDA\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      warnings.warn(\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'cython_bbox' extension\n",
      "    creating build\n",
      "    creating build\\temp.win-amd64-3.9\n",
      "    creating build\\temp.win-amd64-3.9\\Release\n",
      "    creating build\\temp.win-amd64-3.9\\Release\\src\n",
      "    \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.33.31629\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -ID:\\Programs\\ANACONDA\\lib\\site-packages\\numpy\\core\\include -ID:\\Programs\\ANACONDA\\include -ID:\\Programs\\ANACONDA\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.33.31629\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.33.31629\\ATLMFC\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /Tcsrc/cython_bbox.c /Fobuild\\temp.win-amd64-3.9\\Release\\src/cython_bbox.obj -Wno-cpp\n",
      "    cl : Command line error D8021 : invalid numeric argument '/Wno-cpp'\n",
      "    error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.33.31629\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'D:\\Programs\\ANACONDA\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\kools\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2spob2lx\\\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\kools\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2spob2lx\\\\cython-bbox_8acf22d821db4708a445e23f6895b35d\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\kools\\AppData\\Local\\Temp\\pip-record-bafmvxig\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\Programs\\ANACONDA\\Include\\cython-bbox' Check the logs for full command output.\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lap\n",
      "  Using cached lap-0.4.0.tar.gz (1.5 MB)\n",
      "Collecting loguru\n",
      "  Using cached loguru-0.7.0-py3-none-any.whl (59 kB)\n",
      "Collecting norfair\n",
      "  Using cached norfair-2.2.0-py3-none-any.whl (55 kB)\n",
      "Collecting numpy==1.23.3\n",
      "  Using cached numpy-1.23.3-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: opencv-python in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
      "Requirement already satisfied: scipy in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 7)) (1.7.3)\n",
      "Requirement already satisfied: pyyaml in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 8)) (6.0)\n",
      "Collecting easydict\n",
      "  Using cached easydict-1.10.tar.gz (6.4 kB)\n",
      "Collecting gdown\n",
      "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pandas in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: tabulate in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 12)) (0.8.9)\n",
      "Collecting typing-extensions==3.10.0.2\n",
      "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wheel in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 14)) (0.37.1)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp39-cp39-win_amd64.whl (161.8 MB)\n",
      "Requirement already satisfied: torchvision in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 16)) (0.14.0)\n",
      "Requirement already satisfied: Cython in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 18)) (0.29.28)\n",
      "Requirement already satisfied: ultralytics in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 19)) (8.0.98)\n",
      "Collecting asone-ocr\n",
      "  Using cached asone_ocr-1.6.2-py3-none-any.whl (71.5 MB)\n",
      "Collecting motpy\n",
      "  Using cached motpy-0.0.10-py3-none-any.whl (15 kB)\n",
      "Collecting torchreid==0.2.5\n",
      "  Using cached torchreid-0.2.5.tar.gz (92 kB)\n",
      "Requirement already satisfied: tensorboard in d:\\programs\\anaconda\\lib\\site-packages (from -r requirements.txt (line 24)) (2.10.0)\n",
      "Collecting protobuf==3.20\n",
      "  Using cached protobuf-3.20.0-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.14.1-cp39-cp39-win_amd64.whl (6.5 MB)\n",
      "Collecting coremltools\n",
      "  Using cached coremltools-6.3.0.tar.gz (941 kB)\n",
      "Collecting win32-setctime>=1.0.0\n",
      "  Using cached win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: colorama>=0.3.4 in d:\\programs\\anaconda\\lib\\site-packages (from loguru->-r requirements.txt (line 2)) (0.4.4)\n",
      "Collecting filterpy<2.0.0,>=1.4.5\n",
      "  Using cached filterpy-1.4.5.zip (177 kB)\n",
      "Collecting rich<13.0.0,>=9.10.0\n",
      "  Using cached rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Requirement already satisfied: filelock in d:\\programs\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: six in d:\\programs\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in d:\\programs\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 10)) (2.27.1)\n",
      "Requirement already satisfied: tqdm in d:\\programs\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 10)) (4.64.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\programs\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 10)) (4.11.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programs\\anaconda\\lib\\site-packages (from pandas->-r requirements.txt (line 11)) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programs\\anaconda\\lib\\site-packages (from pandas->-r requirements.txt (line 11)) (2.8.2)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "  Using cached torchvision-0.15.1-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "  Using cached torchvision-0.14.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\programs\\anaconda\\lib\\site-packages (from torchvision->-r requirements.txt (line 16)) (9.0.1)\n",
      "  Downloading torchvision-0.13.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\programs\\anaconda\\lib\\site-packages (from ultralytics->-r requirements.txt (line 19)) (0.11.2)\n",
      "Requirement already satisfied: thop>=0.1.1 in d:\\programs\\anaconda\\lib\\site-packages (from ultralytics->-r requirements.txt (line 19)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: sentry-sdk in d:\\programs\\anaconda\\lib\\site-packages (from ultralytics->-r requirements.txt (line 19)) (1.22.2)\n",
      "Requirement already satisfied: psutil in d:\\programs\\anaconda\\lib\\site-packages (from ultralytics->-r requirements.txt (line 19)) (5.8.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in d:\\programs\\anaconda\\lib\\site-packages (from ultralytics->-r requirements.txt (line 19)) (3.5.1)\n",
      "Collecting Shapely\n",
      "  Downloading shapely-2.0.1-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "Collecting python-bidi\n",
      "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-win_amd64.whl (313 kB)\n",
      "Requirement already satisfied: scikit-image in d:\\programs\\anaconda\\lib\\site-packages (from asone-ocr->-r requirements.txt (line 20)) (0.19.2)\n",
      "Collecting pyclipper\n",
      "  Downloading pyclipper-1.3.0.post4-cp39-cp39-win_amd64.whl (95 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (61.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (1.33.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (2.0.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (1.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (1.42.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\programs\\anaconda\\lib\\site-packages (from tensorboard->-r requirements.txt (line 24)) (1.8.1)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.54.0-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: flatbuffers in d:\\programs\\anaconda\\lib\\site-packages (from onnxruntime->-r requirements.txt (line 27)) (2.0.7)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: packaging in d:\\programs\\anaconda\\lib\\site-packages (from onnxruntime->-r requirements.txt (line 27)) (21.3)\n",
      "Requirement already satisfied: sympy in d:\\programs\\anaconda\\lib\\site-packages (from onnxruntime->-r requirements.txt (line 27)) (1.10.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 24)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 24)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 24)) (4.2.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.18.0-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 24)) (1.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in d:\\programs\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 24)) (1.26.15)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programs\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics->-r requirements.txt (line 19)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programs\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics->-r requirements.txt (line 19)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programs\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics->-r requirements.txt (line 19)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programs\\anaconda\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics->-r requirements.txt (line 19)) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\programs\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 24)) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 10)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 10)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 10)) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 24)) (3.2.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in d:\\programs\\anaconda\\lib\\site-packages (from rich<13.0.0,>=9.10.0->norfair->-r requirements.txt (line 3)) (2.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\programs\\anaconda\\lib\\site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 10)) (2.3.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\programs\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in d:\\programs\\anaconda\\lib\\site-packages (from scikit-image->asone-ocr->-r requirements.txt (line 20)) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.2 in d:\\programs\\anaconda\\lib\\site-packages (from scikit-image->asone-ocr->-r requirements.txt (line 20)) (2.7.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in d:\\programs\\anaconda\\lib\\site-packages (from scikit-image->asone-ocr->-r requirements.txt (line 20)) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in d:\\programs\\anaconda\\lib\\site-packages (from scikit-image->asone-ocr->-r requirements.txt (line 20)) (2021.7.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programs\\anaconda\\lib\\site-packages (from sympy->onnxruntime->-r requirements.txt (line 27)) (1.2.1)\n",
      "Building wheels for collected packages: torchreid, lap, easydict, coremltools, filterpy\n",
      "  Building wheel for torchreid (setup.py): started\n",
      "  Building wheel for torchreid (setup.py): finished with status 'done'\n",
      "  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144345 sha256=96e70f1e1e9546030537782383f8f7d3dd5979d11933e3dbff61dc3740315c30\n",
      "  Stored in directory: c:\\users\\kools\\appdata\\local\\pip\\cache\\wheels\\b7\\59\\ec\\af092297eea5a5fe533a08afe157ccd582d821193fd1c12a32\n",
      "  Building wheel for lap (setup.py): started\n",
      "  Building wheel for lap (setup.py): finished with status 'done'\n",
      "  Created wheel for lap: filename=lap-0.4.0-cp39-cp39-win_amd64.whl size=1469476 sha256=3a15cc3ce78bdf652fa90f6705a58d2ba5b9313ced337e04d9d6776a1fcc407c\n",
      "  Stored in directory: c:\\users\\kools\\appdata\\local\\pip\\cache\\wheels\\2f\\8b\\30\\e7dd4f9dc44fb438381df571c9a6bddc35aafd1bf39c4f8911\n",
      "  Building wheel for easydict (setup.py): started\n",
      "  Building wheel for easydict (setup.py): finished with status 'done'\n",
      "  Created wheel for easydict: filename=easydict-1.10-py3-none-any.whl size=6506 sha256=76d271b88a62166354cb2c732dec3f4daf2764aa3f6c6a59619b70ec9766147b\n",
      "  Stored in directory: c:\\users\\kools\\appdata\\local\\pip\\cache\\wheels\\0d\\9a\\a9\\02f3a5f0c6b2c57184661770360c58db8166f5c877780e98f2\n",
      "  Building wheel for coremltools (setup.py): started\n",
      "  Building wheel for coremltools (setup.py): finished with status 'done'\n",
      "  Created wheel for coremltools: filename=coremltools-6.3.0-py3-none-any.whl size=1220328 sha256=21f54ea4ab143b6e75726ce2338d19e8fc02478c15c29a9d6b7c03be64dce4be\n",
      "  Stored in directory: c:\\users\\kools\\appdata\\local\\pip\\cache\\wheels\\c4\\d9\\07\\21c9bf958992c296738b5e472ff3c06880299998315169d4ac\n",
      "  Building wheel for filterpy (setup.py): started\n",
      "  Building wheel for filterpy (setup.py): finished with status 'done'\n",
      "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=408890dddd72745344d942f127b81e4a1b1d9987fb6e64bf473f6b4baa967c5e\n",
      "  Stored in directory: c:\\users\\kools\\appdata\\local\\pip\\cache\\wheels\\53\\e6\\de\\a09ea01e923aaf88b9f8c7c44329e857b2c1a31901167e55e6\n",
      "Successfully built torchreid lap easydict coremltools filterpy\n",
      "Installing collected packages: typing-extensions, pyreadline3, numpy, torch, scipy, humanfriendly, google-auth, commonmark, win32-setctime, torchvision, tensorboard-data-server, Shapely, rich, python-bidi, pyclipper, protobuf, ninja, grpcio, google-auth-oauthlib, filterpy, coloredlogs, torchreid, tensorboard, onnxruntime, norfair, motpy, loguru, lap, gdown, easydict, coremltools, asone-ocr\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\Programs\\\\ANACONDA\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%cd AS-One\n",
    "!pip install cython-bbox\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b5f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sudo\n",
      "  Downloading sudo-1.0.0-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: sudo\n",
      "Successfully installed sudo-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f99f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twilio in d:\\programs\\anaconda\\lib\\site-packages (7.16.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from twilio) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from twilio) (2.27.1)\n",
      "Requirement already satisfied: pytz in d:\\programs\\anaconda\\lib\\site-packages (from twilio) (2021.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\anaconda\\lib\\site-packages (from requests>=2.0.0->twilio) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\anaconda\\lib\\site-packages (from requests>=2.0.0->twilio) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programs\\anaconda\\lib\\site-packages (from requests>=2.0.0->twilio) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\programs\\anaconda\\lib\\site-packages (from requests>=2.0.0->twilio) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\programs\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install twilio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046b1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.98  Python-3.9.12 torch-1.13.0 CPU\n",
      "Setup complete  (4 CPUs, 8.0 GB RAM, 109.7/111.2 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# %pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d01eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.98  Python-3.9.12 torch-1.13.0 CPU\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "Found https:\\ultralytics.com\\images\\zidane.jpg locally at zidane.jpg\n",
      "image 1/1 D:\\Courses\\GRAD\\NoteBooks\\YOLO V8\\zidane.jpg: 384x640 2 persons, 1 tie, 532.2ms\n",
      "Speed: 126.0ms preprocess, 532.2ms inference, 315.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run inference on an image with YOLOv8n\n",
    "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35b324",
   "metadata": {},
   "source": [
    "# Importing Libreries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efeca5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.yolo.utils.plotting import Annotator\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from numpy import array\n",
    "\n",
    "\n",
    "from twilio.rest import Client\n",
    "account_sid = 'ACbac462ab71411eb8a4d32fc5686251fa'\n",
    "auth_token = '629a0712bab50fcf9c8c621197b5831b'\n",
    "client = Client(account_sid, auth_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21201c3",
   "metadata": {},
   "source": [
    "# Load, Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c213e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "# model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
    "# model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104459ed",
   "metadata": {},
   "source": [
    "# Enable GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ecc792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58054654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba9e125f",
   "metadata": {},
   "source": [
    "# Yolo Pose Est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071bae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 124.7ms\n",
      "Speed: 5.0ms preprocess, 124.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict12\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Use the model to predict the poses in the frame\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(frame,save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m (results)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#-------------------------------------------------------------------------------------------------------\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------pose estimation model---------------------------------------------\n",
    "# Define the video capture device\n",
    "cap = cv2.VideoCapture(\"video (1).avi\")\n",
    "\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "\n",
    "    \n",
    "    # Stop the program if no frame is read\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Use the model to predict the poses in the frame\n",
    "    results = model.predict(frame,save=True)\n",
    "    detections = results[0].plot()\n",
    "    print (results)\n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    boxes = results[0].boxes\n",
    "#     box = boxes[0]\n",
    "#     print(boxes.xywh[0,2])\n",
    "#     print(boxes.xywh[3])\n",
    "#--------------------------------------------------------------\n",
    "#     print(boxes.xywhn.type())\n",
    "#     print(results.boxes.xyxy)\n",
    "    \n",
    "#     print(boxes.xywh.type())\n",
    "#the output is torch.FloatTensor, the line below converts it to numpy array\n",
    "#     print(boxes.xywh.cpu().detach().numpy())\n",
    "#the output is torch.FloatTensor, the line below converts it to list\n",
    "\n",
    "    torch_to_list = boxes.xywh.tolist()\n",
    "#     print(torch_to_list)\n",
    "  \n",
    "    \n",
    "#--------------checking the diminsion of the array    \n",
    "    a = array(torch_to_list)\n",
    "#     print(torch_to_list)\n",
    "#     print (a.shape)\n",
    "# --------------------------------------------------\n",
    "#dont let the model make calculations before the regon of interest found...\n",
    "#     if  a.shape == (1,4):\n",
    "#         print(torch_to_list[0][2],torch_to_list[0][3])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    " \n",
    "    \n",
    "    # Create a copy of the frame without the boundary boxes\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "\n",
    "\n",
    "# If the distance to the ground is below a threshold, then the person has fallen\n",
    "#     if  a.shape == (1,4):\n",
    "#         distance_to_ground = torch_to_list[0][3] #Hight\n",
    "#         keypoints_width = torch_to_list[0][2] #width\n",
    "        \n",
    "# #         print(torch_to_list[0][2],torch_to_list[0][3])\n",
    "#         if distance_to_ground < keypoints_width:\n",
    "        \n",
    "#             print(\"Fall detected!\")\n",
    "    #             cv2.rectangle(frame_copy, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # Display the resulting image with the predicted poses\n",
    "#     cv2.imshow('Pose Detection', frame_copy)  \n",
    "    cv2.imshow('YOLOv8 Pose Detection', detections)\n",
    "\n",
    "    \n",
    "    # Exit the program if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture device and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a3986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b242958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c45c8372",
   "metadata": {},
   "source": [
    "# With Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219e12ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 165.6ms\n",
      "Speed: 5.0ms preprocess, 165.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 207.9ms\n",
      "Speed: 21.9ms preprocess, 207.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 209.0ms\n",
      "Speed: 3.0ms preprocess, 209.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 216.5ms\n",
      "Speed: 6.0ms preprocess, 216.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 221.4ms\n",
      "Speed: 15.1ms preprocess, 221.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 198.9ms\n",
      "Speed: 3.0ms preprocess, 198.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 166.0ms\n",
      "Speed: 4.0ms preprocess, 166.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 (no detections), 244.3ms\n",
      "Speed: 8.0ms preprocess, 244.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 243.3ms\n",
      "Speed: 3.0ms preprocess, 243.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 177.5ms\n",
      "Speed: 4.0ms preprocess, 177.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 206.0ms\n",
      "Speed: 7.0ms preprocess, 206.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 216.0ms\n",
      "Speed: 3.0ms preprocess, 216.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 173.9ms\n",
      "Speed: 3.0ms preprocess, 173.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 186.0ms\n",
      "Speed: 7.0ms preprocess, 186.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 181.9ms\n",
      "Speed: 3.0ms preprocess, 181.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 234.4ms\n",
      "Speed: 3.0ms preprocess, 234.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 264.3ms\n",
      "Speed: 4.0ms preprocess, 264.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 163.6ms\n",
      "Speed: 3.0ms preprocess, 163.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 166.1ms\n",
      "Speed: 4.5ms preprocess, 166.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 179.5ms\n",
      "Speed: 3.0ms preprocess, 179.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 180.9ms\n",
      "Speed: 4.0ms preprocess, 180.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 203.0ms\n",
      "Speed: 4.0ms preprocess, 203.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 268.9ms\n",
      "Speed: 9.0ms preprocess, 268.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 242.8ms\n",
      "Speed: 4.0ms preprocess, 242.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 390.7ms\n",
      "Speed: 20.0ms preprocess, 390.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 254.9ms\n",
      "Speed: 4.0ms preprocess, 254.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 237.9ms\n",
      "Speed: 3.0ms preprocess, 237.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 359.0ms\n",
      "Speed: 7.0ms preprocess, 359.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 323.1ms\n",
      "Speed: 4.0ms preprocess, 323.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 295.2ms\n",
      "Speed: 3.0ms preprocess, 295.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 271.3ms\n",
      "Speed: 7.0ms preprocess, 271.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 413.9ms\n",
      "Speed: 4.0ms preprocess, 413.9ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 450.2ms\n",
      "Speed: 23.1ms preprocess, 450.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 344.2ms\n",
      "Speed: 5.0ms preprocess, 344.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 321.0ms\n",
      "Speed: 5.0ms preprocess, 321.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 189.9ms\n",
      "Speed: 4.0ms preprocess, 189.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 234.7ms\n",
      "Speed: 3.0ms preprocess, 234.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 248.9ms\n",
      "Speed: 4.0ms preprocess, 248.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 218.8ms\n",
      "Speed: 4.0ms preprocess, 218.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 221.8ms\n",
      "Speed: 3.0ms preprocess, 221.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 207.7ms\n",
      "Speed: 5.0ms preprocess, 207.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 229.9ms\n",
      "Speed: 6.0ms preprocess, 229.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 208.9ms\n",
      "Speed: 4.0ms preprocess, 208.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 231.0ms\n",
      "Speed: 6.0ms preprocess, 231.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 228.0ms\n",
      "Speed: 5.0ms preprocess, 228.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 215.0ms\n",
      "Speed: 4.0ms preprocess, 215.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 205.0ms\n",
      "Speed: 4.0ms preprocess, 205.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 218.9ms\n",
      "Speed: 3.2ms preprocess, 218.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 203.0ms\n",
      "Speed: 4.0ms preprocess, 203.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 227.0ms\n",
      "Speed: 4.0ms preprocess, 227.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 216.0ms\n",
      "Speed: 5.0ms preprocess, 216.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 210.0ms\n",
      "Speed: 7.0ms preprocess, 210.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 242.0ms\n",
      "Speed: 3.0ms preprocess, 242.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 204.0ms\n",
      "Speed: 3.0ms preprocess, 204.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n",
      "0: 480x640 1 person, 220.0ms\n",
      "Speed: 2.0ms preprocess, 220.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 251.0ms\n",
      "Speed: 5.0ms preprocess, 251.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 240.0ms\n",
      "Speed: 6.0ms preprocess, 240.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 220.0ms\n",
      "Speed: 3.0ms preprocess, 220.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 217.0ms\n",
      "Speed: 4.0ms preprocess, 217.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 191.5ms\n",
      "Speed: 3.0ms preprocess, 191.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 249.8ms\n",
      "Speed: 3.0ms preprocess, 249.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 203.5ms\n",
      "Speed: 8.0ms preprocess, 203.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 214.4ms\n",
      "Speed: 4.0ms preprocess, 214.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 199.5ms\n",
      "Speed: 3.0ms preprocess, 199.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 286.5ms\n",
      "Speed: 3.5ms preprocess, 286.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 218.0ms\n",
      "Speed: 4.0ms preprocess, 218.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 199.0ms\n",
      "Speed: 4.0ms preprocess, 199.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 216.9ms\n",
      "Speed: 3.0ms preprocess, 216.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 207.9ms\n",
      "Speed: 5.0ms preprocess, 207.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 215.9ms\n",
      "Speed: 3.0ms preprocess, 215.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 274.7ms\n",
      "Speed: 3.0ms preprocess, 274.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 319.8ms\n",
      "Speed: 3.0ms preprocess, 319.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 332.9ms\n",
      "Speed: 4.0ms preprocess, 332.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 289.0ms\n",
      "Speed: 5.0ms preprocess, 289.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 288.0ms\n",
      "Speed: 6.0ms preprocess, 288.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 326.5ms\n",
      "Speed: 4.5ms preprocess, 326.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 326.0ms\n",
      "Speed: 6.0ms preprocess, 326.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 328.0ms\n",
      "Speed: 5.0ms preprocess, 328.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 317.0ms\n",
      "Speed: 7.0ms preprocess, 317.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 316.0ms\n",
      "Speed: 14.0ms preprocess, 316.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 311.0ms\n",
      "Speed: 4.0ms preprocess, 311.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 277.0ms\n",
      "Speed: 4.0ms preprocess, 277.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 372.1ms\n",
      "Speed: 4.0ms preprocess, 372.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 315.1ms\n",
      "Speed: 3.0ms preprocess, 315.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 317.0ms\n",
      "Speed: 3.0ms preprocess, 317.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 408.0ms\n",
      "Speed: 4.0ms preprocess, 408.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 317.0ms\n",
      "Speed: 9.0ms preprocess, 317.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 249.9ms\n",
      "Speed: 6.1ms preprocess, 249.9ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 258.0ms\n",
      "Speed: 9.0ms preprocess, 258.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 232.0ms\n",
      "Speed: 5.0ms preprocess, 232.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 228.1ms\n",
      "Speed: 6.0ms preprocess, 228.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 222.1ms\n",
      "Speed: 3.0ms preprocess, 222.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 220.0ms\n",
      "Speed: 6.0ms preprocess, 220.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 286.0ms\n",
      "Speed: 6.0ms preprocess, 286.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 225.0ms\n",
      "Speed: 3.0ms preprocess, 225.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 238.0ms\n",
      "Speed: 4.0ms preprocess, 238.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 240.0ms\n",
      "Speed: 3.5ms preprocess, 240.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detected!\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------pose estimation model---------------------------------------------\n",
    "# Define the video capture device\n",
    "# cap = cv2.VideoCapture(\"video (2).avi\")\n",
    "source =\"video (1).avi\" \n",
    "# source =\"sleep.mp4\" \n",
    "cap = cv2.VideoCapture(source)\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "\n",
    "\n",
    "#------------------------Saving the output to the server side--------------------------------------------\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "\n",
    "# THE skelaton video\n",
    "result_Public = cv2.VideoWriter('C:\\\\Users\\\\kools\\\\Downloads\\\\Grad\\\\YOLO V8\\\\OUT\\\\public_skelton.mp4', fourcc, 10, (640,480))\n",
    "\n",
    "# result_Public = cv2.VideoWriter(\"C:\\\\Users\\\\kools\\\\Downloads\\\\Grad\\\\YOLO V8\\\\OUT\\\\public_skelton.avi\", \n",
    "#                          cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "#                          10, (640,480))\n",
    "\n",
    "# THE RGB video\n",
    "\n",
    "result_private = cv2.VideoWriter('C:\\\\Users\\\\kools\\\\Downloads\\\\Grad\\\\YOLO V8\\\\OUT\\\\private_RGB.mp4', fourcc, 10, (640,480))\n",
    "\n",
    "\n",
    "# result_private = cv2.VideoWriter(\"C:\\\\Users\\\\kools\\\\Downloads\\\\Grad\\\\YOLO V8\\\\OUT\\\\private_RGB.avi\", \n",
    "#                          cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "#                          10, (640,480))\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#----------------------\n",
    "fall_counter = 0\n",
    "#----------------------\n",
    "\n",
    "\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "\n",
    "    \n",
    "    # Stop the program if no frame is read\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Use the model to predict the poses in the frame\n",
    "    results = model.predict(frame,save=True)\n",
    "    detections = results[0].plot()\n",
    "    #     print (results[0].keypoints)\n",
    "\n",
    "\n",
    "\n",
    "    #---------------------------------------MOG-----------------------------\n",
    "\n",
    "    #     grey = cv2.copyTo(grayImage,frame)\n",
    "    mog_first = mog2.apply(frame)\n",
    "    #     cv.imshow('frame',fgmask)\n",
    "    #------------------------------------------------\n",
    "\n",
    "    #----------other methods similar to mog----------\n",
    " \n",
    "    mog_detections0 = cv2.copyTo(frame,mog_first)\n",
    "    #     denoised_2 = cv2.blur(mog_detections0, (15, 15))\n",
    "    #     grayImage = cv2.cvtColor(mog_detections0, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    grayFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    (thresh, blackAndWhiteFrame) = cv2.threshold(grayFrame, 70, 255, cv2.THRESH_BINARY)\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     mog_detections = cv2.copyTo(frame,grayImage)\n",
    "\n",
    "\n",
    "#     mog_detections = mog2.apply(frame)\n",
    "#     results = model.predict(frame,save=True)\n",
    "#     mog_detections = cv2.addWeighted(mog_frame,0.5,detections,0.7,0)\n",
    "#     mog_detections =mog_frame + results[0].plot()\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "  \n",
    "    \n",
    "    \n",
    "    boxes = results[0].boxes\n",
    "#     box = boxes[0]\n",
    "#     print(boxes.xywh[0,2])\n",
    "#     print(boxes.xywh[3])\n",
    "    #--------------------------------------------------------------\n",
    "#     print(boxes.xywhn.type())\n",
    "#     print(results.boxes.xyxy)\n",
    "    \n",
    "#     print(boxes.xywh.type())\n",
    "#the output is torch.FloatTensor, the line below converts it to numpy array\n",
    "#     print(boxes.xywh.cpu().detach().numpy())\n",
    "#the output is torch.FloatTensor, the line below converts it to list\n",
    "\n",
    "    torch_to_list = boxes.xywh.tolist()\n",
    "#     print(torch_to_list)\n",
    "  \n",
    "    \n",
    "    #--------------checking the diminsion of the array-------------    \n",
    "    a = array(torch_to_list)\n",
    "#     print(torch_to_list)\n",
    "#     print (a.shape)\n",
    "    # -------------------------------------------------------------\n",
    "    #dont let the model make calculations before the regon of interest found...\n",
    "#     if  a.shape == (1,4):\n",
    "#         print(torch_to_list[0][2],torch_to_list[0][3])\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    " \n",
    "    \n",
    "    # Create a copy of the frame without the boundary boxes\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "\n",
    "    #----------------------------------DETECTING FALLS------------------------------------\n",
    "    # If the distance to the ground is below a threshold, then the person has fallen\n",
    "    if  a.shape == (1,4):\n",
    "        distance_to_ground = torch_to_list[0][3] #Hight\n",
    "        keypoints_width = torch_to_list[0][2] #width\n",
    "        \n",
    "#         print(torch_to_list[0][2],torch_to_list[0][3])\n",
    "        if distance_to_ground < keypoints_width and source !=\"sleep.mp4\" :\n",
    "        \n",
    "            print(\"Fall detected!\")\n",
    "            fall_counter = fall_counter + 1\n",
    "            \n",
    "            if fall_counter == 5:\n",
    "                client.messages.create(body = \"Fall Detected, Please check your Application..\",from_ = '+14067807611', to='+201014454866')\n",
    "\n",
    "            \n",
    "            \n",
    "            for r in results:\n",
    "                annotator = Annotator(frame)\n",
    "\n",
    "                boxes = r.boxes\n",
    "                for box in boxes:\n",
    "                        b = box.xyxy[0]  # top, left, bottom, right) format\n",
    "                        c = box.cls\n",
    "                        annotator.box_label(b, \"Fall Detected\")\n",
    "\n",
    "                frame = annotator.result()  \n",
    "\n",
    "\n",
    "        else:\n",
    "            for r in results:\n",
    "                annotator = Annotator(frame)\n",
    "\n",
    "                boxes = r.boxes\n",
    "                for box in boxes:\n",
    "                        b = box.xyxy[0]  #(top, left, bottom, right) format\n",
    "                        c = box.cls\n",
    "                        annotator.box_label(b, \"Normal\")\n",
    "\n",
    "                frame = annotator.result()  \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Display the resulting image with the predicted poses\n",
    "#     cv2.imshow('Pose Detection', frame_copy)  \n",
    "\n",
    "\n",
    "    #---------------------------------output_streams----------------------------------\n",
    "    #--------------------------cashing---------------------\n",
    "    #sending the image to the cache to prevent np problem \n",
    "    cv2.imwrite('C:\\\\Users\\\\kools\\\\Downloads\\\\Grad\\\\YOLO V8\\\\Cache\\\\temp.jpg', blackAndWhiteFrame)\n",
    "    public_stream = cv2.imread('C:\\\\Users\\\\kools\\\\Downloads\\\\Grad\\\\YOLO V8\\\\Cache\\\\temp.jpg')\n",
    "\n",
    "    \n",
    "    #------------------------------------------------------\n",
    "    #sending the output video to the app\n",
    " \n",
    "    result_Public.write(public_stream) \n",
    "    result_private.write(frame_copy)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    cv2.imshow('Pose estimation Fall Detection', frame)\n",
    "    cv2.imshow('skeleton ', detections)\n",
    "    cv2.imshow('MOG ', blackAndWhiteFrame)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Exit the program if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture device and close the display window\n",
    "result_Public.release()\n",
    "result_private.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9d7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872469ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
